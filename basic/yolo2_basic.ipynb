{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-05T09:15:02.790442Z",
     "start_time": "2017-07-05T17:14:59.548756+08:00"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import SGD, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "global IMAGE_H, IMAGE_W\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-05T09:19:58.442938Z",
     "start_time": "2017-07-05T17:19:58.214533+08:00"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exec(open(\"./utils.py\").read())\n",
    "\n",
    "NORM_H, NORM_W = 416, 416\n",
    "IMAGE_H, IMAGE_W =  416, 416\n",
    "\n",
    "GRID_H, GRID_W = 13 , 13\n",
    "BATCH_SIZE = 8\n",
    "BOX = 5\n",
    "\n",
    "LABELS = ['1', '2']\n",
    "CLASS = len(LABELS)\n",
    "\n",
    "\n",
    "THRESHOLD = 0.2\n",
    "\n",
    "ANCHORS          = [149,74, 194,97, 282,141, 392,196, 511,255]\n",
    "\n",
    "SCALE_NOOB, SCALE_CONF, SCALE_COOR, SCALE_PROB = 0.5, 5.0, 5.0, 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wt_path = 'yolo.weights'                      \n",
    "img_dir = 'data/train/'\n",
    "train_ann = 'data/train_ann.json'\n",
    "\n",
    "val_img_dir = 'data/val/'\n",
    "val_ann = 'data/val_ann.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_ann) as f:\n",
    "    anns = json.load(f)\n",
    "\n",
    "with open(val_ann) as f:\n",
    "    val_anns = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv2D(16, (3,3), strides=(1,1), padding='same', use_bias=False, input_shape=(416,416,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 2 - 5\n",
    "for i in range(0,4):\n",
    "    model.add(Conv2D(32*(2**i), (3,3), strides=(1,1), padding='same', use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 6\n",
    "model.add(Conv2D(512, (3,3), strides=(1,1), padding='same', use_bias=False))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(1,1), padding='same'))\n",
    "\n",
    "# # Layer 7 - 8\n",
    "# for _ in range(0,2):\n",
    "#     model.add(Conv2D(1024, (3,3), strides=(1,1), padding='same', use_bias=False))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(LeakyReLU(alpha=0.1))\n",
    "\n",
    "# Layer 9\n",
    "model.add(Conv2D(BOX * (4 + 1 + CLASS), (1, 1), strides=(1, 1), kernel_initializer='he_normal'))\n",
    "model.add(Activation('linear'))\n",
    "model.add(Reshape((GRID_H, GRID_W, BOX, 4 + 1 + CLASS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T11:54:51.338391Z",
     "start_time": "2017-06-29T19:54:51.257520+08:00"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    ### Adjust prediction\n",
    "    # adjust x and y      \n",
    "    pred_box_xy = tf.sigmoid(y_pred[:,:,:,:,:2])\n",
    "    \n",
    "    # adjust w and h\n",
    "    pred_box_wh = tf.exp(y_pred[:,:,:,:,2:4]) * np.reshape(ANCHORS, [1,1,1,BOX,2])\n",
    "    pred_box_wh = tf.sqrt(pred_box_wh / np.reshape([float(GRID_W), float(GRID_H)], [1,1,1,1,2]))\n",
    "    \n",
    "    # adjust confidence\n",
    "    pred_box_conf = tf.expand_dims(tf.sigmoid(y_pred[:, :, :, :, 4]), -1)\n",
    "    \n",
    "    # adjust probability\n",
    "    pred_box_prob = tf.nn.softmax(y_pred[:, :, :, :, 5:])\n",
    "    \n",
    "    y_pred = tf.concat([pred_box_xy, pred_box_wh, pred_box_conf, pred_box_prob], 4)\n",
    "    print(\"Y_pred shape: {}\".format(y_pred.shape))\n",
    "    \n",
    "    ### Adjust ground truth\n",
    "    # adjust x and y\n",
    "    center_xy = .5*(y_true[:,:,:,:,0:2] + y_true[:,:,:,:,2:4])\n",
    "    center_xy = center_xy / np.reshape([(float(NORM_W)/GRID_W), (float(NORM_H)/GRID_H)], [1,1,1,1,2])\n",
    "    true_box_xy = center_xy - tf.floor(center_xy)\n",
    "    \n",
    "    # adjust w and h\n",
    "    true_box_wh = (y_true[:,:,:,:,2:4] - y_true[:,:,:,:,0:2])\n",
    "    true_box_wh = tf.sqrt(true_box_wh / np.reshape([float(NORM_W), float(NORM_H)], [1,1,1,1,2]))\n",
    "    \n",
    "    # adjust confidence\n",
    "    pred_tem_wh = tf.pow(pred_box_wh, 2) * np.reshape([GRID_W, GRID_H], [1,1,1,1,2])\n",
    "    pred_box_area = pred_tem_wh[:,:,:,:,0] * pred_tem_wh[:,:,:,:,1]\n",
    "    pred_box_ul = pred_box_xy - 0.5 * pred_tem_wh\n",
    "    pred_box_bd = pred_box_xy + 0.5 * pred_tem_wh\n",
    "    \n",
    "    true_tem_wh = tf.pow(true_box_wh, 2) * np.reshape([GRID_W, GRID_H], [1,1,1,1,2])\n",
    "    true_box_area = true_tem_wh[:,:,:,:,0] * true_tem_wh[:,:,:,:,1]\n",
    "    true_box_ul = true_box_xy - 0.5 * true_tem_wh\n",
    "    true_box_bd = true_box_xy + 0.5 * true_tem_wh\n",
    "    \n",
    "    intersect_ul = tf.maximum(pred_box_ul, true_box_ul) \n",
    "    intersect_br = tf.minimum(pred_box_bd, true_box_bd)\n",
    "    intersect_wh = intersect_br - intersect_ul\n",
    "    intersect_wh = tf.maximum(intersect_wh, 0.0)\n",
    "    intersect_area = intersect_wh[:,:,:,:,0] * intersect_wh[:,:,:,:,1]\n",
    "    \n",
    "    iou = tf.truediv(intersect_area, true_box_area + pred_box_area - intersect_area)\n",
    "    best_box = tf.equal(iou, tf.reduce_max(iou, [3], True)) \n",
    "    best_box = tf.to_float(best_box)\n",
    "    true_box_conf = tf.expand_dims(best_box * y_true[:,:,:,:,4], -1)\n",
    "    \n",
    "    # adjust confidence\n",
    "    true_box_prob = y_true[:,:,:,:,5:]\n",
    "    \n",
    "    y_true = tf.concat([true_box_xy, true_box_wh, true_box_conf, true_box_prob], 4)\n",
    "    print(\"Y_true shape: {}\".format(y_true.shape))\n",
    "    #y_true = tf.Print(y_true, [true_box_wh], message='DEBUG', summarize=30000)    \n",
    "    \n",
    "    ### Compute the weights\n",
    "    weight_coor = tf.concat(4 * [true_box_conf], 4)\n",
    "    weight_coor = SCALE_COOR * weight_coor\n",
    "    \n",
    "    weight_conf = SCALE_NOOB * (1. - true_box_conf) + SCALE_CONF * true_box_conf\n",
    "    \n",
    "    weight_prob = tf.concat(CLASS * [true_box_conf], 4) \n",
    "    weight_prob = SCALE_PROB * weight_prob \n",
    "    \n",
    "    weight = tf.concat([weight_coor, weight_conf, weight_prob], 4)\n",
    "    print(\"Weight shape: {}\".format(weight.shape))\n",
    "    \n",
    "    ### Finalize the loss\n",
    "    loss = tf.pow(y_pred - y_true, 2)\n",
    "    loss = loss * weight\n",
    "    loss = tf.reshape(loss, [-1, GRID_W*GRID_H*BOX*(4 + 1 + CLASS)])\n",
    "    loss = tf.reduce_sum(loss, 1)\n",
    "    loss = .5 * tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 416, 416, 16)      144       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 416, 416, 16)      64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 416, 416, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 208, 208, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 208, 208, 32)      4608      \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 208, 208, 32)      128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 208, 208, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 104, 104, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 104, 104, 64)      18432     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 104, 104, 64)      256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 104, 104, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 52, 52, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 52, 52, 128)       73728     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 52, 52, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 52, 52, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 256)       294912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 26, 26, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 26, 26, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 13, 13, 512)       1179648   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 13, 13, 512)       2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 13, 13, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 13, 13, 35)        17955     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 13, 13, 35)        0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 13, 13, 5, 7)      0         \n",
      "=================================================================\n",
      "Total params: 1,593,459\n",
      "Trainable params: 1,591,443\n",
      "Non-trainable params: 2,016\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-29T12:13:06.924612Z",
     "start_time": "2017-06-29T20:13:06.918728+08:00"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='loss', min_delta=0.001, patience=3, mode='min', verbose=1)\n",
    "checkpoint = ModelCheckpoint('weights-basic.hdf5', monitor='loss', verbose=1, save_best_only=True, mode='min', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_pred shape: (?, 13, 13, 5, 7)\n",
      "Y_true shape: (?, 13, 13, 5, ?)\n",
      "Weight shape: (?, 13, 13, 5, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vira/Documents/Machine Learning/blackai-challenge/.env/lib/python2.7/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., 235, epochs=1, callbacks=[<keras.ca..., max_queue_size=3, verbose=2)`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 1048s - loss: nan\n",
      "\n",
      "Epoch 00001: loss did not improve from inf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1268bd650>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGD(lr=0.001, decay=0.0005, momentum=0.9)\n",
    "#adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(loss=custom_loss, optimizer=sgd)\n",
    "model.fit_generator(data_gen(anns, BATCH_SIZE), \n",
    "                    int(len(anns)/BATCH_SIZE), \n",
    "                    epochs = 1, \n",
    "                    verbose = 2,\n",
    "                    callbacks = [early_stop, checkpoint],\n",
    "#                     validation_data  = data_gen(val_anns, BATCH_SIZE),\n",
    "                    max_q_size = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform detection on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-05T09:20:19.526888Z",
     "start_time": "2017-07-05T17:20:18.934995+08:00"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e57718deb244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights-basic.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'new_model' is not defined"
     ]
    }
   ],
   "source": [
    "new_model.load_weights(\"weights-basic.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open(\"./utils.py\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-07-05T09:20:37.331274Z",
     "start_time": "2017-07-05T17:20:36.669840+08:00"
    }
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('imagenet_samples/ILSVRC2014_train_00000124.JPEG')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "input_image = cv2.resize(image, (416, 416))\n",
    "input_image = input_image / 255.\n",
    "input_image = input_image[:,:,::-1]\n",
    "input_image = np.expand_dims(input_image, 0)\n",
    "\n",
    "netout = new_model.predict(input_image)\n",
    "\n",
    "#print netout\n",
    "image = interpret_netout(image, netout[0])\n",
    "plt.imshow(image[:,:,::-1]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "381px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "674px",
    "left": "0px",
    "right": "1096px",
    "top": "73px",
    "width": "253px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
